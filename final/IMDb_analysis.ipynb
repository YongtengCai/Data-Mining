{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDb 文本情感分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## RNN分析——深度学习初体验（坐牢）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available.\n"
     ]
    }
   ],
   "source": [
    "#测试cuda是否可用\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available.\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#导入数据\n",
    "import pandas as pd\n",
    "data = pd.read_csv('E:/本科/数据挖掘与商务分析/hw/final/IMDB Dataset.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将数据集分为训练集和测试集\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = data['review']\n",
    "y = data['sentiment']\n",
    "X_train_valid, X_test, y_train_valid, y_test = train_test_split(X, y, test_size=0.5, random_state=1)\n",
    "test_dataset = [(x, y) for x, y in zip(X_test, y_test)]\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_valid, y_train_valid, test_size=0.2, random_state=1)\n",
    "train_dataset = [(x, y) for x, y in zip(X_train, y_train)]\n",
    "valid_dataset = [(x, y) for x, y in zip(X_valid, y_valid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab-size: 71093\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "token_counts = Counter()\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) +\\\n",
    "        ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = text.split()\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "for review, _ in train_dataset:\n",
    "    tokens = tokenizer(review)\n",
    "    token_counts.update(tokens)\n",
    " \n",
    "    \n",
    "print('Vocab-size:', len(token_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 7, 35, 479]\n"
     ]
    }
   ],
   "source": [
    "#利用torchtext的vocab将token转换为整数\n",
    "from torchtext.vocab import vocab\n",
    "\n",
    "sorted_by_freq_tuples = sorted(token_counts.items(), key=lambda x: x[1], reverse=True) \n",
    "ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
    "\n",
    "vocab = vocab(ordered_dict)\n",
    "vocab.insert_token(\"<pad>\", 0)\n",
    "vocab.insert_token(\"<unk>\", 1)\n",
    "vocab.set_default_index(1)\n",
    "\n",
    "print([vocab[token] for token in ['this', 'is', 'an', 'example']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义转换函数\n",
    "import torch.nn as nn\n",
    "device = torch.device('cuda' )\n",
    "review_pipeline = lambda x: [vocab[token] for token in tokenizer(x)]\n",
    "sentiment_pipeline = lambda x: 1 if x == 'positive' else 0\n",
    "\n",
    "def collate_batch(batch):\n",
    "    review_list, sentiment_list, lengths = [], [], []\n",
    "    for _review, _sentiment,in batch:\n",
    "        processed_text =torch.tensor(review_pipeline(_review), dtype=torch.int64)\n",
    "        review_list.append(processed_text)\n",
    "        sentiment_list.append(sentiment_pipeline(_sentiment))\n",
    "        lengths.append(processed_text.size(0))\n",
    "    sentiment_list = torch.tensor(sentiment_list)\n",
    "    lengths = torch.tensor(lengths)\n",
    "    padded_review_list = nn.utils.rnn.pad_sequence(review_list, batch_first=True)\n",
    "    return padded_review_list.to(device), sentiment_list.to(device), lengths.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   16, 20203,  4622,  ...,     0,     0,     0],\n",
      "        [43943,     7,     3,  ...,     0,     0,     0],\n",
      "        [   10,   142,     3,  ...,  1092,     6,    31],\n",
      "        [  310,  1691,  2179,  ...,     0,     0,     0]], device='cuda:0')\n",
      "tensor([0, 1, 1, 1], device='cuda:0')\n",
      "tensor([270, 364, 416, 113], device='cuda:0')\n",
      "torch.Size([4, 416])\n"
     ]
    }
   ],
   "source": [
    "#small batch 看看实力\n",
    "from torch.utils.data import DataLoader\n",
    "dataloader = DataLoader(train_dataset, batch_size=4, shuffle=False, collate_fn=collate_batch)\n",
    "review_batch, sentiment_batch, length_batch = next(iter(dataloader))\n",
    "print(review_batch)\n",
    "print(sentiment_batch)\n",
    "print(length_batch)\n",
    "print(review_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch data生成data loader\n",
    "batch_size = 32 \n",
    "\n",
    "train_dl = DataLoader(train_dataset, batch_size=batch_size,shuffle=True, collate_fn=collate_batch)\n",
    "valid_dl = DataLoader(valid_dataset, batch_size=batch_size,shuffle=False, collate_fn=collate_batch)\n",
    "test_dl = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.8105, -0.2065, -0.2479],\n",
      "         [ 1.4421, -1.2474, -0.9963],\n",
      "         [-0.3907,  0.7674, -0.4990],\n",
      "         [ 0.5550, -1.4660, -0.2243]],\n",
      "\n",
      "        [[-0.3907,  0.7674, -0.4990],\n",
      "         [-0.5652,  0.3689, -0.9838],\n",
      "         [ 1.4421, -1.2474, -0.9963],\n",
      "         [ 0.0000,  0.0000,  0.0000]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#将句子转为嵌入编码\n",
    "embedding = nn.Embedding(num_embeddings=10, embedding_dim=3, padding_idx=0)\n",
    "\n",
    "#example\n",
    "text_encoded_input = torch.LongTensor([[1,2,4,5],[4,3,2,0]])\n",
    "print(embedding(text_encoded_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建循环神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(71095, 20, padding_idx=0)\n",
       "  (rnn): LSTM(20, 64, batch_first=True)\n",
       "  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#用RNN进行情感分析\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0) \n",
    "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size,  batch_first=True)\n",
    "        self.fc1 = nn.Linear(rnn_hidden_size, fc_hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(fc_hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, text, lengths):\n",
    "        out = self.embedding(text)\n",
    "        out = nn.utils.rnn.pack_padded_sequence(out, lengths.cpu().numpy(), enforce_sorted=False, batch_first=True)\n",
    "        out, (hidden, cell) = self.rnn(out)\n",
    "        out = hidden[-1, :, :]\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "embed_dim = 20\n",
    "rnn_hidden_size = 64\n",
    "fc_hidden_size = 64\n",
    "\n",
    "torch.manual_seed(1)\n",
    "model = RNN(vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size) \n",
    "model = model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#编写train函数\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_loss = 0, 0\n",
    "    for text_batch, label_batch, lengths in dataloader:\n",
    "        text_batch = text_batch.to(device)\n",
    "        label_batch = label_batch.to(device).float()  # 确保标签是浮点类型\n",
    "        lengths = lengths.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(text_batch, lengths)[:, 0]\n",
    "        loss = loss_fn(pred, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_acc += ((pred>=0.5).float() == label_batch).float().sum().item()\n",
    "        total_loss += loss.item()*label_batch.size(0)\n",
    "    return total_acc/len(dataloader.dataset), total_loss/len(dataloader.dataset)\n",
    " \n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_loss = 0, 0\n",
    "    all_labels, all_preds = [], []\n",
    "    with torch.no_grad():\n",
    "        for text_batch, label_batch, lengths in dataloader:\n",
    "            text_batch = text_batch.to(device)\n",
    "            label_batch = label_batch.to(device).float()  # 确保标签是浮点类型\n",
    "            lengths = lengths.to(device)\n",
    "            pred = model(text_batch, lengths)[:, 0]\n",
    "            loss = loss_fn(pred, label_batch)\n",
    "            total_acc += ((pred>=0.5).float() == label_batch).float().sum().item()\n",
    "            total_loss += loss.item()*label_batch.size(0)\n",
    "            # 收集所有预测分数和真实标签\n",
    "            all_labels.extend(label_batch.cpu().numpy())\n",
    "            all_preds.extend(pred.cpu().numpy())\n",
    "    return total_acc/len(dataloader.dataset), total_loss/len(dataloader.dataset), all_labels, all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 accuracy: 0.5987 val_accuracy: 0.5022\n",
      "Epoch 1 accuracy: 0.7109 val_accuracy: 0.7166\n",
      "Epoch 2 accuracy: 0.7269 val_accuracy: 0.7724\n",
      "Epoch 3 accuracy: 0.8294 val_accuracy: 0.8200\n",
      "Epoch 4 accuracy: 0.8887 val_accuracy: 0.8234\n",
      "Epoch 5 accuracy: 0.9138 val_accuracy: 0.8274\n",
      "Epoch 6 accuracy: 0.9375 val_accuracy: 0.8600\n",
      "Epoch 7 accuracy: 0.9526 val_accuracy: 0.8650\n",
      "Epoch 8 accuracy: 0.9678 val_accuracy: 0.8706\n",
      "Epoch 9 accuracy: 0.9778 val_accuracy: 0.8656\n"
     ]
    }
   ],
   "source": [
    "#编写损失函数\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10 \n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "#开始显卡炼丹 \n",
    "for epoch in range(num_epochs):\n",
    "    acc_train, loss_train = train(train_dl)\n",
    "    acc_valid, loss_valid,_,_ = evaluate(valid_dl)\n",
    "    print(f'Epoch {epoch} accuracy: {acc_train:.4f} val_accuracy: {acc_valid:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.864\n",
      "Recall: 0.889\n",
      "F1 Score: 0.876\n",
      "AUC Score: 0.941\n"
     ]
    }
   ],
   "source": [
    "#用sklearn评估测试集效果\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "acc_test, loss_test, true_labels, pred_scores= evaluate(test_dl)\n",
    "\n",
    "# 二分类的阈值设置为0.5\n",
    "pred_labels = [1 if score >= 0.5 else 0 for score in pred_scores]\n",
    "precision = precision_score(true_labels, pred_labels)\n",
    "recall = recall_score(true_labels, pred_labels)\n",
    "f1 = f1_score(true_labels, pred_labels)\n",
    "auc = roc_auc_score(true_labels, pred_scores)\n",
    "print(f'Precision: {precision:.3f}')\n",
    "print(f'Recall: {recall:.3f}')\n",
    "print(f'F1 Score: {f1:.3f}')\n",
    "print(f'AUC Score: {auc:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 双向循环神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(71095, 20, padding_idx=0)\n",
       "  (rnn): LSTM(20, 64, batch_first=True, bidirectional=True)\n",
       "  (fc1): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#用RNN进行情感分析\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0) \n",
    "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.fc1 = nn.Linear(rnn_hidden_size*2, fc_hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(fc_hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, text, lengths):\n",
    "        out = self.embedding(text)\n",
    "        out = nn.utils.rnn.pack_padded_sequence(out, lengths.cpu().numpy(), enforce_sorted=False, batch_first=True)\n",
    "        out, (hidden, cell) = self.rnn(out)\n",
    "        out = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "embed_dim = 20\n",
    "rnn_hidden_size = 64\n",
    "fc_hidden_size = 64\n",
    "\n",
    "torch.manual_seed(1)\n",
    "model = RNN(vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size) \n",
    "model = model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#编写train函数\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_loss = 0, 0\n",
    "    for text_batch, label_batch, lengths in dataloader:\n",
    "        text_batch = text_batch.to(device)\n",
    "        label_batch = label_batch.to(device).float()  # 确保标签是浮点类型\n",
    "        lengths = lengths.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(text_batch, lengths)[:, 0]\n",
    "        loss = loss_fn(pred, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_acc += ((pred>=0.5).float() == label_batch).float().sum().item()\n",
    "        total_loss += loss.item()*label_batch.size(0)\n",
    "    return total_acc/len(dataloader.dataset), total_loss/len(dataloader.dataset)\n",
    " \n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_loss = 0, 0\n",
    "    all_labels, all_preds = [], []\n",
    "    with torch.no_grad():\n",
    "        for text_batch, label_batch, lengths in dataloader:\n",
    "            text_batch = text_batch.to(device)\n",
    "            label_batch = label_batch.to(device).float()  # 确保标签是浮点类型\n",
    "            lengths = lengths.to(device)\n",
    "            pred = model(text_batch, lengths)[:, 0]\n",
    "            loss = loss_fn(pred, label_batch)\n",
    "            total_acc += ((pred>=0.5).float() == label_batch).float().sum().item()\n",
    "            total_loss += loss.item()*label_batch.size(0)\n",
    "            # 收集所有预测分数和真实标签\n",
    "            all_labels.extend(label_batch.cpu().numpy())\n",
    "            all_preds.extend(pred.cpu().numpy())\n",
    "    return total_acc/len(dataloader.dataset), total_loss/len(dataloader.dataset), all_labels, all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 accuracy: 0.6117 val_accuracy: 0.6420\n",
      "Epoch 1 accuracy: 0.6414 val_accuracy: 0.7438\n",
      "Epoch 2 accuracy: 0.8025 val_accuracy: 0.8212\n",
      "Epoch 3 accuracy: 0.8752 val_accuracy: 0.8482\n",
      "Epoch 4 accuracy: 0.9109 val_accuracy: 0.8534\n",
      "Epoch 5 accuracy: 0.9301 val_accuracy: 0.8362\n",
      "Epoch 6 accuracy: 0.9507 val_accuracy: 0.8698\n",
      "Epoch 7 accuracy: 0.9637 val_accuracy: 0.8652\n",
      "Epoch 8 accuracy: 0.9772 val_accuracy: 0.8670\n",
      "Epoch 9 accuracy: 0.9850 val_accuracy: 0.8718\n"
     ]
    }
   ],
   "source": [
    "#编写损失函数\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10 \n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "#开始显卡炼丹 \n",
    "for epoch in range(num_epochs):\n",
    "    acc_train, loss_train = train(train_dl)\n",
    "    acc_valid, loss_valid,_,_ = evaluate(valid_dl)\n",
    "    print(f'Epoch {epoch} accuracy: {acc_train:.4f} val_accuracy: {acc_valid:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.884\n",
      "Recall: 0.871\n",
      "F1 Score: 0.877\n",
      "AUC Score: 0.943\n"
     ]
    }
   ],
   "source": [
    "#用sklearn评估测试集效果\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "acc_test, loss_test, true_labels, pred_scores= evaluate(test_dl)\n",
    "\n",
    "# 二分类的阈值设置为0.5\n",
    "pred_labels = [1 if score >= 0.5 else 0 for score in pred_scores]\n",
    "precision = precision_score(true_labels, pred_labels)\n",
    "recall = recall_score(true_labels, pred_labels)\n",
    "f1 = f1_score(true_labels, pred_labels)\n",
    "auc = roc_auc_score(true_labels, pred_scores)\n",
    "print(f'Precision: {precision:.3f}')\n",
    "print(f'Recall: {recall:.3f}')\n",
    "print(f'F1 Score: {f1:.3f}')\n",
    "print(f'AUC Score: {auc:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 增加神经网络层数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(71095, 20, padding_idx=0)\n",
       "  (rnn1): LSTM(20, 64, batch_first=True, bidirectional=True)\n",
       "  (rnn2): LSTM(128, 64, batch_first=True, bidirectional=True)\n",
       "  (fc1): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (fc3): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.rnn1 = nn.LSTM(embed_dim, rnn_hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.rnn2 = nn.LSTM(rnn_hidden_size*2, rnn_hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.fc1 = nn.Linear(rnn_hidden_size*2, fc_hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(fc_hidden_size, fc_hidden_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(fc_hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, text, lengths):\n",
    "        out = self.embedding(text)\n",
    "        out = nn.utils.rnn.pack_padded_sequence(out, lengths.cpu().numpy(), enforce_sorted=False, batch_first=True)\n",
    "        out, (hidden, cell) = self.rnn1(out)\n",
    "        out, _ = self.rnn2(out)  # 这里需要使用pad_packed_sequence来恢复序列\n",
    "        out = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)  # 添加这行代码\n",
    "        out, output = out  # 现在out是恢复后的序列，output是序列的隐藏状态\n",
    "        out = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)  # 这里需要确保hidden的形状正确\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "embed_dim = 20\n",
    "rnn_hidden_size = 64\n",
    "fc_hidden_size = 64\n",
    "\n",
    "torch.manual_seed(1)\n",
    "model = RNN(vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size) \n",
    "model = model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#编写train函数\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_loss = 0, 0\n",
    "    for text_batch, label_batch, lengths in dataloader:\n",
    "        text_batch = text_batch.to(device)\n",
    "        label_batch = label_batch.to(device).float()  # 确保标签是浮点类型\n",
    "        lengths = lengths.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(text_batch, lengths)[:, 0]\n",
    "        loss = loss_fn(pred, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_acc += ((pred>=0.5).float() == label_batch).float().sum().item()\n",
    "        total_loss += loss.item()*label_batch.size(0)\n",
    "    return total_acc/len(dataloader.dataset), total_loss/len(dataloader.dataset)\n",
    " \n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_loss = 0, 0\n",
    "    all_labels, all_preds = [], []\n",
    "    with torch.no_grad():\n",
    "        for text_batch, label_batch, lengths in dataloader:\n",
    "            text_batch = text_batch.to(device)\n",
    "            label_batch = label_batch.to(device).float()  # 确保标签是浮点类型\n",
    "            lengths = lengths.to(device)\n",
    "            pred = model(text_batch, lengths)[:, 0]\n",
    "            loss = loss_fn(pred, label_batch)\n",
    "            total_acc += ((pred>=0.5).float() == label_batch).float().sum().item()\n",
    "            total_loss += loss.item()*label_batch.size(0)\n",
    "            # 收集所有预测分数和真实标签\n",
    "            all_labels.extend(label_batch.cpu().numpy())\n",
    "            all_preds.extend(pred.cpu().numpy())\n",
    "    return total_acc/len(dataloader.dataset), total_loss/len(dataloader.dataset), all_labels, all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 accuracy: 0.6068 val_accuracy: 0.6564\n",
      "Epoch 1 accuracy: 0.7601 val_accuracy: 0.7704\n",
      "Epoch 2 accuracy: 0.8211 val_accuracy: 0.7826\n",
      "Epoch 3 accuracy: 0.8323 val_accuracy: 0.7664\n",
      "Epoch 4 accuracy: 0.8773 val_accuracy: 0.8264\n",
      "Epoch 5 accuracy: 0.9125 val_accuracy: 0.8250\n",
      "Epoch 6 accuracy: 0.9324 val_accuracy: 0.8554\n",
      "Epoch 7 accuracy: 0.9447 val_accuracy: 0.8568\n",
      "Epoch 8 accuracy: 0.9615 val_accuracy: 0.8594\n",
      "Epoch 9 accuracy: 0.9688 val_accuracy: 0.8636\n",
      "Epoch 10 accuracy: 0.9777 val_accuracy: 0.8532\n",
      "Epoch 11 accuracy: 0.9849 val_accuracy: 0.8554\n",
      "Epoch 12 accuracy: 0.9836 val_accuracy: 0.8492\n",
      "Epoch 13 accuracy: 0.9890 val_accuracy: 0.8608\n",
      "Epoch 14 accuracy: 0.9948 val_accuracy: 0.8552\n",
      "Epoch 15 accuracy: 0.9918 val_accuracy: 0.8562\n",
      "Epoch 16 accuracy: 0.9967 val_accuracy: 0.8574\n",
      "Epoch 17 accuracy: 0.9954 val_accuracy: 0.8540\n",
      "Epoch 18 accuracy: 0.9939 val_accuracy: 0.8578\n",
      "Epoch 19 accuracy: 0.9963 val_accuracy: 0.8620\n"
     ]
    }
   ],
   "source": [
    "#编写损失函数\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10 \n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "#开始显卡炼丹 \n",
    "for epoch in range(num_epochs):\n",
    "    acc_train, loss_train = train(train_dl)\n",
    "    acc_valid, loss_valid,_,_ = evaluate(valid_dl)\n",
    "    print(f'Epoch {epoch} accuracy: {acc_train:.4f} val_accuracy: {acc_valid:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.846\n",
      "Recall: 0.893\n",
      "F1 Score: 0.869\n",
      "AUC Score: 0.934\n"
     ]
    }
   ],
   "source": [
    "#用sklearn评估测试集效果\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "acc_test, loss_test, true_labels, pred_scores= evaluate(test_dl)\n",
    "\n",
    "# 二分类的阈值设置为0.5\n",
    "pred_labels = [1 if score >= 0.5 else 0 for score in pred_scores]\n",
    "precision = precision_score(true_labels, pred_labels)\n",
    "recall = recall_score(true_labels, pred_labels)\n",
    "f1 = f1_score(true_labels, pred_labels)\n",
    "auc = roc_auc_score(true_labels, pred_scores)\n",
    "print(f'Precision: {precision:.3f}')\n",
    "print(f'Recall: {recall:.3f}')\n",
    "print(f'F1 Score: {f1:.3f}')\n",
    "print(f'AUC Score: {auc:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可能由于过拟合问题，难以进一步提升"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
