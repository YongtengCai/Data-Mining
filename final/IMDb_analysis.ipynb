{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDb 文本情感分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## RNN分析——深度学习初体验（坐牢）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available.\n"
     ]
    }
   ],
   "source": [
    "#测试cuda是否可用\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available.\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#导入数据\n",
    "import pandas as pd\n",
    "data = pd.read_csv('E:/本科/数据挖掘与商务分析/hw/final/IMDB Dataset.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将数据集分为训练集和测试集\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = data['review']\n",
    "y = data['sentiment']\n",
    "X_train_valid, X_test, y_train_valid, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "test_dataset = [(x, y) for x, y in zip(X_test, y_test)]\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_valid, y_train_valid, test_size=0.2, random_state=1)\n",
    "train_dataset = [(x, y) for x, y in zip(X_train, y_train)]\n",
    "valid_dataset = [(x, y) for x, y in zip(X_valid, y_valid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab-size: 86341\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "token_counts = Counter()\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) +\\\n",
    "        ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = text.split()\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "for review, _ in train_dataset:\n",
    "    tokens = tokenizer(review)\n",
    "    token_counts.update(tokens)\n",
    " \n",
    "    \n",
    "print('Vocab-size:', len(token_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 7, 35, 467]\n"
     ]
    }
   ],
   "source": [
    "#利用torchtext的vocab将token转换为整数\n",
    "from torchtext.vocab import vocab\n",
    "\n",
    "sorted_by_freq_tuples = sorted(token_counts.items(), key=lambda x: x[1], reverse=True) \n",
    "ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
    "\n",
    "vocab = vocab(ordered_dict)\n",
    "vocab.insert_token(\"<pad>\", 0)\n",
    "vocab.insert_token(\"<unk>\", 1)\n",
    "vocab.set_default_index(1)\n",
    "\n",
    "print([vocab[token] for token in ['this', 'is', 'an', 'example']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义转换函数\n",
    "import torch.nn as nn\n",
    "device = torch.device('cuda' )\n",
    "review_pipeline = lambda x: [vocab[token] for token in tokenizer(x)]\n",
    "sentiment_pipeline = lambda x: 1 if x == 'positive' else 0\n",
    "\n",
    "def collate_batch(batch):\n",
    "    review_list, sentiment_list, lengths = [], [], []\n",
    "    for _review, _sentiment,in batch:\n",
    "        processed_text =torch.tensor(review_pipeline(_review), dtype=torch.int64)\n",
    "        review_list.append(processed_text)\n",
    "        sentiment_list.append(sentiment_pipeline(_sentiment))\n",
    "        lengths.append(processed_text.size(0))\n",
    "    sentiment_list = torch.tensor(sentiment_list)\n",
    "    lengths = torch.tensor(lengths)\n",
    "    padded_review_list = nn.utils.rnn.pad_sequence(review_list, batch_first=True)\n",
    "    return padded_review_list.to(device), sentiment_list.to(device), lengths.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   11,     7,    39,  ...,     0,     0,     0],\n",
      "        [   10,  1304,     6,  ...,     0,     0,     0],\n",
      "        [    2,   590,  2348,  ..., 25611,   785, 29777],\n",
      "        [   10,   103,    11,  ...,     0,     0,     0]], device='cuda:0')\n",
      "tensor([1, 1, 0, 1], device='cuda:0')\n",
      "tensor([164, 120, 448, 254], device='cuda:0')\n",
      "torch.Size([4, 448])\n"
     ]
    }
   ],
   "source": [
    "#small batch 看看实力\n",
    "from torch.utils.data import DataLoader\n",
    "dataloader = DataLoader(train_dataset, batch_size=4, shuffle=False, collate_fn=collate_batch)\n",
    "review_batch, sentiment_batch, length_batch = next(iter(dataloader))\n",
    "print(review_batch)\n",
    "print(sentiment_batch)\n",
    "print(length_batch)\n",
    "print(review_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch data生成data loader\n",
    "batch_size = 32 \n",
    "\n",
    "train_dl = DataLoader(train_dataset, batch_size=batch_size,shuffle=True, collate_fn=collate_batch)\n",
    "valid_dl = DataLoader(valid_dataset, batch_size=batch_size,shuffle=False, collate_fn=collate_batch)\n",
    "test_dl = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.8969, -0.1245,  1.3060],\n",
      "         [ 0.4696,  0.0376, -0.4505],\n",
      "         [-1.2924,  0.7151,  0.7477],\n",
      "         [-0.8698,  1.5329, -0.5256]],\n",
      "\n",
      "        [[-1.2924,  0.7151,  0.7477],\n",
      "         [-1.3677, -1.6807,  0.2428],\n",
      "         [ 0.4696,  0.0376, -0.4505],\n",
      "         [ 0.0000,  0.0000,  0.0000]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#将句子转为嵌入编码\n",
    "embedding = nn.Embedding(num_embeddings=10, embedding_dim=3, padding_idx=0)\n",
    "\n",
    "#example\n",
    "text_encoded_input = torch.LongTensor([[1,2,4,5],[4,3,2,0]])\n",
    "print(embedding(text_encoded_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建循环神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(86343, 20, padding_idx=0)\n",
       "  (rnn): LSTM(20, 64, batch_first=True)\n",
       "  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#用RNN进行情感分析\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0) \n",
    "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size,  batch_first=True)\n",
    "        self.fc1 = nn.Linear(rnn_hidden_size, fc_hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(fc_hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, text, lengths):\n",
    "        out = self.embedding(text)\n",
    "        out = nn.utils.rnn.pack_padded_sequence(out, lengths.cpu().numpy(), enforce_sorted=False, batch_first=True)\n",
    "        out, (hidden, cell) = self.rnn(out)\n",
    "        out = hidden[-1, :, :]\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "embed_dim = 20\n",
    "rnn_hidden_size = 64\n",
    "fc_hidden_size = 64\n",
    "\n",
    "torch.manual_seed(1)\n",
    "model = RNN(vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size) \n",
    "model = model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#编写train函数\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_loss = 0, 0\n",
    "    for text_batch, label_batch, lengths in dataloader:\n",
    "        text_batch = text_batch.to(device)\n",
    "        label_batch = label_batch.to(device).float()  # 确保标签是浮点类型\n",
    "        lengths = lengths.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(text_batch, lengths)[:, 0]\n",
    "        loss = loss_fn(pred, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_acc += ((pred>=0.5).float() == label_batch).float().sum().item()\n",
    "        total_loss += loss.item()*label_batch.size(0)\n",
    "    return total_acc/len(dataloader.dataset), total_loss/len(dataloader.dataset)\n",
    " \n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_loss = 0, 0\n",
    "    all_labels, all_preds = [], []\n",
    "    with torch.no_grad():\n",
    "        for text_batch, label_batch, lengths in dataloader:\n",
    "            text_batch = text_batch.to(device)\n",
    "            label_batch = label_batch.to(device).float()  # 确保标签是浮点类型\n",
    "            lengths = lengths.to(device)\n",
    "            pred = model(text_batch, lengths)[:, 0]\n",
    "            loss = loss_fn(pred, label_batch)\n",
    "            total_acc += ((pred>=0.5).float() == label_batch).float().sum().item()\n",
    "            total_loss += loss.item()*label_batch.size(0)\n",
    "            # 收集所有预测分数和真实标签\n",
    "            all_labels.extend(label_batch.cpu().numpy())\n",
    "            all_preds.extend(pred.cpu().numpy())\n",
    "    return total_acc/len(dataloader.dataset), total_loss/len(dataloader.dataset), all_labels, all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 accuracy: 0.5945 val_accuracy: 0.6593\n",
      "Epoch 1 accuracy: 0.6984 val_accuracy: 0.7422\n",
      "Epoch 2 accuracy: 0.7612 val_accuracy: 0.8031\n",
      "Epoch 3 accuracy: 0.8486 val_accuracy: 0.8464\n",
      "Epoch 4 accuracy: 0.8827 val_accuracy: 0.8664\n",
      "Epoch 5 accuracy: 0.9051 val_accuracy: 0.8744\n",
      "Epoch 6 accuracy: 0.9263 val_accuracy: 0.8784\n",
      "Epoch 7 accuracy: 0.9401 val_accuracy: 0.8769\n",
      "Epoch 8 accuracy: 0.9527 val_accuracy: 0.8814\n",
      "Epoch 9 accuracy: 0.9631 val_accuracy: 0.8855\n"
     ]
    }
   ],
   "source": [
    "#编写损失函数\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10 \n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "#开始显卡炼丹 \n",
    "for epoch in range(num_epochs):\n",
    "    acc_train, loss_train = train(train_dl)\n",
    "    acc_valid, loss_valid,_,_ = evaluate(valid_dl)\n",
    "    print(f'Epoch {epoch} accuracy: {acc_train:.4f} val_accuracy: {acc_valid:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.858\n",
      "Recall: 0.934\n",
      "F1 Score: 0.894\n",
      "AUC Score: 0.955\n"
     ]
    }
   ],
   "source": [
    "#用sklearn评估测试集效果\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "acc_test, loss_test, true_labels, pred_scores= evaluate(test_dl)\n",
    "\n",
    "# 二分类的阈值设置为0.5\n",
    "pred_labels = [1 if score >= 0.5 else 0 for score in pred_scores]\n",
    "precision = precision_score(true_labels, pred_labels)\n",
    "recall = recall_score(true_labels, pred_labels)\n",
    "f1 = f1_score(true_labels, pred_labels)\n",
    "auc = roc_auc_score(true_labels, pred_scores)\n",
    "print(f'Precision: {precision:.3f}')\n",
    "print(f'Recall: {recall:.3f}')\n",
    "print(f'F1 Score: {f1:.3f}')\n",
    "print(f'AUC Score: {auc:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 双向循环神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(86343, 20, padding_idx=0)\n",
       "  (rnn): LSTM(20, 64, batch_first=True, bidirectional=True)\n",
       "  (fc1): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#用RNN进行情感分析\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0) \n",
    "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.fc1 = nn.Linear(rnn_hidden_size*2, fc_hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(fc_hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, text, lengths):\n",
    "        out = self.embedding(text)\n",
    "        out = nn.utils.rnn.pack_padded_sequence(out, lengths.cpu().numpy(), enforce_sorted=False, batch_first=True)\n",
    "        out, (hidden, cell) = self.rnn(out)\n",
    "        out = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "embed_dim = 20\n",
    "rnn_hidden_size = 64\n",
    "fc_hidden_size = 64\n",
    "\n",
    "torch.manual_seed(1)\n",
    "model = RNN(vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size) \n",
    "model = model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#编写train函数\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_loss = 0, 0\n",
    "    for text_batch, label_batch, lengths in dataloader:\n",
    "        text_batch = text_batch.to(device)\n",
    "        label_batch = label_batch.to(device).float()  # 确保标签是浮点类型\n",
    "        lengths = lengths.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(text_batch, lengths)[:, 0]\n",
    "        loss = loss_fn(pred, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_acc += ((pred>=0.5).float() == label_batch).float().sum().item()\n",
    "        total_loss += loss.item()*label_batch.size(0)\n",
    "    return total_acc/len(dataloader.dataset), total_loss/len(dataloader.dataset)\n",
    " \n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_loss = 0, 0\n",
    "    all_labels, all_preds = [], []\n",
    "    with torch.no_grad():\n",
    "        for text_batch, label_batch, lengths in dataloader:\n",
    "            text_batch = text_batch.to(device)\n",
    "            label_batch = label_batch.to(device).float()  # 确保标签是浮点类型\n",
    "            lengths = lengths.to(device)\n",
    "            pred = model(text_batch, lengths)[:, 0]\n",
    "            loss = loss_fn(pred, label_batch)\n",
    "            total_acc += ((pred>=0.5).float() == label_batch).float().sum().item()\n",
    "            total_loss += loss.item()*label_batch.size(0)\n",
    "            # 收集所有预测分数和真实标签\n",
    "            all_labels.extend(label_batch.cpu().numpy())\n",
    "            all_preds.extend(pred.cpu().numpy())\n",
    "    return total_acc/len(dataloader.dataset), total_loss/len(dataloader.dataset), all_labels, all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 accuracy: 0.6147 val_accuracy: 0.7330\n",
      "Epoch 1 accuracy: 0.7090 val_accuracy: 0.7538\n",
      "Epoch 2 accuracy: 0.8048 val_accuracy: 0.8373\n",
      "Epoch 3 accuracy: 0.8787 val_accuracy: 0.8520\n",
      "Epoch 4 accuracy: 0.9123 val_accuracy: 0.8761\n",
      "Epoch 5 accuracy: 0.9342 val_accuracy: 0.8782\n",
      "Epoch 6 accuracy: 0.9509 val_accuracy: 0.8818\n",
      "Epoch 7 accuracy: 0.9643 val_accuracy: 0.8534\n",
      "Epoch 8 accuracy: 0.9765 val_accuracy: 0.8779\n",
      "Epoch 9 accuracy: 0.9850 val_accuracy: 0.8816\n"
     ]
    }
   ],
   "source": [
    "#编写损失函数\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10 \n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "#开始显卡炼丹 \n",
    "for epoch in range(num_epochs):\n",
    "    acc_train, loss_train = train(train_dl)\n",
    "    acc_valid, loss_valid,_,_ = evaluate(valid_dl)\n",
    "    print(f'Epoch {epoch} accuracy: {acc_train:.4f} val_accuracy: {acc_valid:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.901\n",
      "Recall: 0.865\n",
      "F1 Score: 0.883\n",
      "AUC Score: 0.952\n"
     ]
    }
   ],
   "source": [
    "#用sklearn评估测试集效果\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "acc_test, loss_test, true_labels, pred_scores= evaluate(test_dl)\n",
    "\n",
    "# 二分类的阈值设置为0.5\n",
    "pred_labels = [1 if score >= 0.5 else 0 for score in pred_scores]\n",
    "precision = precision_score(true_labels, pred_labels)\n",
    "recall = recall_score(true_labels, pred_labels)\n",
    "f1 = f1_score(true_labels, pred_labels)\n",
    "auc = roc_auc_score(true_labels, pred_scores)\n",
    "print(f'Precision: {precision:.3f}')\n",
    "print(f'Recall: {recall:.3f}')\n",
    "print(f'F1 Score: {f1:.3f}')\n",
    "print(f'AUC Score: {auc:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 增加神经网络层数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(86343, 20, padding_idx=0)\n",
       "  (rnn1): LSTM(20, 64, batch_first=True, bidirectional=True)\n",
       "  (rnn2): LSTM(128, 64, batch_first=True, bidirectional=True)\n",
       "  (fc1): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (fc3): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.rnn1 = nn.LSTM(embed_dim, rnn_hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.rnn2 = nn.LSTM(rnn_hidden_size*2, rnn_hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.fc1 = nn.Linear(rnn_hidden_size*2, fc_hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(fc_hidden_size, fc_hidden_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(fc_hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, text, lengths):\n",
    "        out = self.embedding(text)\n",
    "        out = nn.utils.rnn.pack_padded_sequence(out, lengths.cpu().numpy(), enforce_sorted=False, batch_first=True)\n",
    "        out, (hidden, cell) = self.rnn1(out)\n",
    "        out, _ = self.rnn2(out)  # 这里需要使用pad_packed_sequence来恢复序列\n",
    "        out = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)  # 添加这行代码\n",
    "        out, output = out  # 现在out是恢复后的序列，output是序列的隐藏状态\n",
    "        out = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)  # 这里需要确保hidden的形状正确\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "embed_dim = 20\n",
    "rnn_hidden_size = 64\n",
    "fc_hidden_size = 64\n",
    "\n",
    "torch.manual_seed(1)\n",
    "model = RNN(vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size) \n",
    "model = model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#编写train函数\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_loss = 0, 0\n",
    "    for text_batch, label_batch, lengths in dataloader:\n",
    "        text_batch = text_batch.to(device)\n",
    "        label_batch = label_batch.to(device).float()  # 确保标签是浮点类型\n",
    "        lengths = lengths.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(text_batch, lengths)[:, 0]\n",
    "        loss = loss_fn(pred, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_acc += ((pred>=0.5).float() == label_batch).float().sum().item()\n",
    "        total_loss += loss.item()*label_batch.size(0)\n",
    "    return total_acc/len(dataloader.dataset), total_loss/len(dataloader.dataset)\n",
    " \n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_loss = 0, 0\n",
    "    all_labels, all_preds = [], []\n",
    "    with torch.no_grad():\n",
    "        for text_batch, label_batch, lengths in dataloader:\n",
    "            text_batch = text_batch.to(device)\n",
    "            label_batch = label_batch.to(device).float()  # 确保标签是浮点类型\n",
    "            lengths = lengths.to(device)\n",
    "            pred = model(text_batch, lengths)[:, 0]\n",
    "            loss = loss_fn(pred, label_batch)\n",
    "            total_acc += ((pred>=0.5).float() == label_batch).float().sum().item()\n",
    "            total_loss += loss.item()*label_batch.size(0)\n",
    "            # 收集所有预测分数和真实标签\n",
    "            all_labels.extend(label_batch.cpu().numpy())\n",
    "            all_preds.extend(pred.cpu().numpy())\n",
    "    return total_acc/len(dataloader.dataset), total_loss/len(dataloader.dataset), all_labels, all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 accuracy: 0.6402 val_accuracy: 0.6529\n",
      "Epoch 1 accuracy: 0.7530 val_accuracy: 0.7700\n",
      "Epoch 2 accuracy: 0.7940 val_accuracy: 0.8285\n",
      "Epoch 3 accuracy: 0.8718 val_accuracy: 0.8660\n",
      "Epoch 4 accuracy: 0.9062 val_accuracy: 0.8819\n",
      "Epoch 5 accuracy: 0.9309 val_accuracy: 0.8862\n",
      "Epoch 6 accuracy: 0.9469 val_accuracy: 0.8910\n",
      "Epoch 7 accuracy: 0.9608 val_accuracy: 0.8851\n",
      "Epoch 8 accuracy: 0.9722 val_accuracy: 0.8825\n",
      "Epoch 9 accuracy: 0.9821 val_accuracy: 0.8882\n"
     ]
    }
   ],
   "source": [
    "#编写损失函数\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10 \n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "#开始显卡炼丹 \n",
    "for epoch in range(num_epochs):\n",
    "    acc_train, loss_train = train(train_dl)\n",
    "    acc_valid, loss_valid,_,_ = evaluate(valid_dl)\n",
    "    print(f'Epoch {epoch} accuracy: {acc_train:.4f} val_accuracy: {acc_valid:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.906\n",
      "Recall: 0.881\n",
      "F1 Score: 0.893\n",
      "AUC Score: 0.955\n"
     ]
    }
   ],
   "source": [
    "#用sklearn评估测试集效果\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "acc_test, loss_test, true_labels, pred_scores= evaluate(test_dl)\n",
    "\n",
    "# 二分类的阈值设置为0.5\n",
    "pred_labels = [1 if score >= 0.5 else 0 for score in pred_scores]\n",
    "precision = precision_score(true_labels, pred_labels)\n",
    "recall = recall_score(true_labels, pred_labels)\n",
    "f1 = f1_score(true_labels, pred_labels)\n",
    "auc = roc_auc_score(true_labels, pred_scores)\n",
    "print(f'Precision: {precision:.3f}')\n",
    "print(f'Recall: {recall:.3f}')\n",
    "print(f'F1 Score: {f1:.3f}')\n",
    "print(f'AUC Score: {auc:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可能由于过拟合问题，难以进一步提升"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 引入Transformer——Attention is all you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.deterministic = True #确保结果可重现\n",
    "torch.manual_seed(1)\n",
    "num_epochs = 3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
