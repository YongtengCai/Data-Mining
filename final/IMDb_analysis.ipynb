{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDb 文本情感分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## RNN分析——深度学习初体验（坐牢）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available.\n"
     ]
    }
   ],
   "source": [
    "#测试cuda是否可用\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available.\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#导入数据\n",
    "import pandas as pd\n",
    "data = pd.read_csv('E:/本科/数据挖掘与商务分析/hw/final/IMDB Dataset.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将数据集分为训练集和测试集\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = data['review']\n",
    "y = data['sentiment']\n",
    "X_train_valid, X_test, y_train_valid, y_test = train_test_split(X, y, test_size=0.5, random_state=1)\n",
    "test_dataset = [(x, y) for x, y in zip(X_test, y_test)]\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_valid, y_train_valid, test_size=0.2, random_state=1)\n",
    "train_dataset = [(x, y) for x, y in zip(X_train, y_train)]\n",
    "valid_dataset = [(x, y) for x, y in zip(X_valid, y_valid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab-size: 71093\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "token_counts = Counter()\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) +\\\n",
    "        ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = text.split()\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "for review, _ in train_dataset:\n",
    "    tokens = tokenizer(review)\n",
    "    token_counts.update(tokens)\n",
    " \n",
    "    \n",
    "print('Vocab-size:', len(token_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 7, 35, 479]\n"
     ]
    }
   ],
   "source": [
    "#利用torchtext的vocab将token转换为整数\n",
    "from torchtext.vocab import vocab\n",
    "\n",
    "sorted_by_freq_tuples = sorted(token_counts.items(), key=lambda x: x[1], reverse=True) \n",
    "ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
    "\n",
    "vocab = vocab(ordered_dict)\n",
    "vocab.insert_token(\"<pad>\", 0)\n",
    "vocab.insert_token(\"<unk>\", 1)\n",
    "vocab.set_default_index(1)\n",
    "\n",
    "print([vocab[token] for token in ['this', 'is', 'an', 'example']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义转换函数\n",
    "import torch.nn as nn\n",
    "device = torch.device('cuda' )\n",
    "review_pipeline = lambda x: [vocab[token] for token in tokenizer(x)]\n",
    "sentiment_pipeline = lambda x: 1 if x == 'positive' else 0\n",
    "\n",
    "def collate_batch(batch):\n",
    "    review_list, sentiment_list, lengths = [], [], []\n",
    "    for _review, _sentiment,in batch:\n",
    "        processed_text =torch.tensor(review_pipeline(_review), dtype=torch.int64)\n",
    "        review_list.append(processed_text)\n",
    "        sentiment_list.append(sentiment_pipeline(_sentiment))\n",
    "        lengths.append(processed_text.size(0))\n",
    "    sentiment_list = torch.tensor(sentiment_list)\n",
    "    lengths = torch.tensor(lengths)\n",
    "    padded_review_list = nn.utils.rnn.pad_sequence(review_list, batch_first=True)\n",
    "    return padded_review_list.to(device), sentiment_list.to(device), lengths.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   16, 20203,  4622,  ...,     0,     0,     0],\n",
      "        [43943,     7,     3,  ...,     0,     0,     0],\n",
      "        [   10,   142,     3,  ...,  1092,     6,    31],\n",
      "        [  310,  1691,  2179,  ...,     0,     0,     0]], device='cuda:0')\n",
      "tensor([0, 1, 1, 1], device='cuda:0')\n",
      "tensor([270, 364, 416, 113], device='cuda:0')\n",
      "torch.Size([4, 416])\n"
     ]
    }
   ],
   "source": [
    "#small batch 看看实力\n",
    "from torch.utils.data import DataLoader\n",
    "dataloader = DataLoader(train_dataset, batch_size=4, shuffle=False, collate_fn=collate_batch)\n",
    "review_batch, sentiment_batch, length_batch = next(iter(dataloader))\n",
    "print(review_batch)\n",
    "print(sentiment_batch)\n",
    "print(length_batch)\n",
    "print(review_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch data生成data loader\n",
    "batch_size = 32 \n",
    "\n",
    "train_dl = DataLoader(train_dataset, batch_size=batch_size,shuffle=True, collate_fn=collate_batch)\n",
    "valid_dl = DataLoader(valid_dataset, batch_size=batch_size,shuffle=False, collate_fn=collate_batch)\n",
    "test_dl = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 2.7666,  1.4503, -1.3632],\n",
      "         [-1.6674,  1.0552,  0.3405],\n",
      "         [ 0.0287, -0.1377,  0.5142],\n",
      "         [-0.2761, -0.0186, -2.4675]],\n",
      "\n",
      "        [[ 0.0287, -0.1377,  0.5142],\n",
      "         [-0.5876, -1.4813, -0.6294],\n",
      "         [-1.6674,  1.0552,  0.3405],\n",
      "         [ 0.0000,  0.0000,  0.0000]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#将句子转为嵌入编码\n",
    "embedding = nn.Embedding(num_embeddings=10, embedding_dim=3, padding_idx=0)\n",
    "\n",
    "#example\n",
    "text_encoded_input = torch.LongTensor([[1,2,4,5],[4,3,2,0]])\n",
    "print(embedding(text_encoded_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建循环神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(71095, 20, padding_idx=0)\n",
       "  (rnn): LSTM(20, 64, batch_first=True)\n",
       "  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#用RNN进行情感分析\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, \n",
    "                                      embed_dim, \n",
    "                                      padding_idx=0) \n",
    "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size, \n",
    "                           batch_first=True)\n",
    "        self.fc1 = nn.Linear(rnn_hidden_size, fc_hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(fc_hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, text, lengths):\n",
    "        out = self.embedding(text)\n",
    "        out = nn.utils.rnn.pack_padded_sequence(out, lengths.cpu().numpy(), enforce_sorted=False, batch_first=True)\n",
    "        out, (hidden, cell) = self.rnn(out)\n",
    "        out = hidden[-1, :, :]\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "embed_dim = 20\n",
    "rnn_hidden_size = 64\n",
    "fc_hidden_size = 64\n",
    "\n",
    "torch.manual_seed(1)\n",
    "model = RNN(vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size) \n",
    "model = model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#编写train函数\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_loss = 0, 0\n",
    "    for text_batch, label_batch, lengths in dataloader:\n",
    "        text_batch = text_batch.to(device)\n",
    "        label_batch = label_batch.to(device).float()  # 确保标签是浮点类型\n",
    "        lengths = lengths.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(text_batch, lengths)[:, 0]\n",
    "        loss = loss_fn(pred, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_acc += ((pred>=0.5).float() == label_batch).float().sum().item()\n",
    "        total_loss += loss.item()*label_batch.size(0)\n",
    "    return total_acc/len(dataloader.dataset), total_loss/len(dataloader.dataset)\n",
    " \n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_loss = 0, 0\n",
    "    all_labels, all_preds = [], []\n",
    "    with torch.no_grad():\n",
    "        for text_batch, label_batch, lengths in dataloader:\n",
    "            text_batch = text_batch.to(device)\n",
    "            label_batch = label_batch.to(device).float()  # 确保标签是浮点类型\n",
    "            lengths = lengths.to(device)\n",
    "            pred = model(text_batch, lengths)[:, 0]\n",
    "            loss = loss_fn(pred, label_batch)\n",
    "            total_acc += ((pred>=0.5).float() == label_batch).float().sum().item()\n",
    "            total_loss += loss.item()*label_batch.size(0)\n",
    "            # 收集所有预测分数和真实标签\n",
    "            all_labels.extend(label_batch.cpu().numpy())\n",
    "            all_preds.extend(pred.cpu().numpy())\n",
    "    return total_acc/len(dataloader.dataset), total_loss/len(dataloader.dataset), all_labels, all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 accuracy: 0.5987 val_accuracy: 0.5022\n",
      "Epoch 1 accuracy: 0.7109 val_accuracy: 0.7166\n",
      "Epoch 2 accuracy: 0.7269 val_accuracy: 0.7724\n",
      "Epoch 3 accuracy: 0.8294 val_accuracy: 0.8200\n",
      "Epoch 4 accuracy: 0.8887 val_accuracy: 0.8234\n",
      "Epoch 5 accuracy: 0.9138 val_accuracy: 0.8274\n",
      "Epoch 6 accuracy: 0.9375 val_accuracy: 0.8600\n",
      "Epoch 7 accuracy: 0.9526 val_accuracy: 0.8650\n",
      "Epoch 8 accuracy: 0.9678 val_accuracy: 0.8706\n",
      "Epoch 9 accuracy: 0.9778 val_accuracy: 0.8656\n",
      "Epoch 10 accuracy: 0.9846 val_accuracy: 0.8672\n",
      "Epoch 11 accuracy: 0.9905 val_accuracy: 0.8628\n",
      "Epoch 12 accuracy: 0.9919 val_accuracy: 0.8540\n",
      "Epoch 13 accuracy: 0.9946 val_accuracy: 0.8622\n",
      "Epoch 14 accuracy: 0.9951 val_accuracy: 0.8608\n",
      "Epoch 15 accuracy: 0.9959 val_accuracy: 0.8596\n",
      "Epoch 16 accuracy: 0.9967 val_accuracy: 0.8642\n",
      "Epoch 17 accuracy: 0.9980 val_accuracy: 0.8648\n",
      "Epoch 18 accuracy: 0.9962 val_accuracy: 0.8658\n",
      "Epoch 19 accuracy: 0.9982 val_accuracy: 0.8618\n",
      "Epoch 20 accuracy: 0.9987 val_accuracy: 0.8596\n",
      "Epoch 21 accuracy: 0.9994 val_accuracy: 0.8686\n",
      "Epoch 22 accuracy: 0.9999 val_accuracy: 0.8668\n",
      "Epoch 23 accuracy: 0.9986 val_accuracy: 0.8642\n",
      "Epoch 24 accuracy: 0.9960 val_accuracy: 0.8662\n",
      "Epoch 25 accuracy: 0.9993 val_accuracy: 0.8650\n",
      "Epoch 26 accuracy: 0.9990 val_accuracy: 0.8630\n",
      "Epoch 27 accuracy: 0.9985 val_accuracy: 0.8642\n",
      "Epoch 28 accuracy: 0.9999 val_accuracy: 0.8658\n",
      "Epoch 29 accuracy: 1.0000 val_accuracy: 0.8662\n"
     ]
    }
   ],
   "source": [
    "#编写损失函数\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10 \n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "#开始显卡炼丹 \n",
    "for epoch in range(num_epochs):\n",
    "    acc_train, loss_train = train(train_dl)\n",
    "    acc_valid, loss_valid,_,_ = evaluate(valid_dl)\n",
    "    print(f'Epoch {epoch} accuracy: {acc_train:.4f} val_accuracy: {acc_valid:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.851\n",
      "Recall: 0.898\n",
      "F1 Score: 0.874\n",
      "AUC Score: 0.932\n"
     ]
    }
   ],
   "source": [
    "#用sklearn评估测试集效果\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "acc_test, loss_test, true_labels, pred_scores= evaluate(test_dl)\n",
    "\n",
    "# 二分类的阈值设置为0.5\n",
    "pred_labels = [1 if score >= 0.5 else 0 for score in pred_scores]\n",
    "precision = precision_score(true_labels, pred_labels)\n",
    "recall = recall_score(true_labels, pred_labels)\n",
    "f1 = f1_score(true_labels, pred_labels)\n",
    "auc = roc_auc_score(true_labels, pred_scores)\n",
    "print(f'Precision: {precision:.3f}')\n",
    "print(f'Recall: {recall:.3f}')\n",
    "print(f'F1 Score: {f1:.3f}')\n",
    "print(f'AUC Score: {auc:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 双向循环神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(71095, 20, padding_idx=0)\n",
       "  (rnn): LSTM(20, 64, batch_first=True, bidirectional=True)\n",
       "  (fc1): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#用RNN进行情感分析\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, \n",
    "                                      embed_dim, \n",
    "                                      padding_idx=0) \n",
    "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size, \n",
    "                           batch_first=True, bidirectional=True)\n",
    "        self.fc1 = nn.Linear(rnn_hidden_size*2, fc_hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(fc_hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, text, lengths):\n",
    "        out = self.embedding(text)\n",
    "        out = nn.utils.rnn.pack_padded_sequence(out, lengths.cpu().numpy(), enforce_sorted=False, batch_first=True)\n",
    "        out, (hidden, cell) = self.rnn(out)\n",
    "        out = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "embed_dim = 20\n",
    "rnn_hidden_size = 64\n",
    "fc_hidden_size = 64\n",
    "\n",
    "torch.manual_seed(1)\n",
    "model = RNN(vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size) \n",
    "model = model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#编写train函数\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_loss = 0, 0\n",
    "    for text_batch, label_batch, lengths in dataloader:\n",
    "        text_batch = text_batch.to(device)\n",
    "        label_batch = label_batch.to(device).float()  # 确保标签是浮点类型\n",
    "        lengths = lengths.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(text_batch, lengths)[:, 0]\n",
    "        loss = loss_fn(pred, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_acc += ((pred>=0.5).float() == label_batch).float().sum().item()\n",
    "        total_loss += loss.item()*label_batch.size(0)\n",
    "    return total_acc/len(dataloader.dataset), total_loss/len(dataloader.dataset)\n",
    " \n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_loss = 0, 0\n",
    "    all_labels, all_preds = [], []\n",
    "    with torch.no_grad():\n",
    "        for text_batch, label_batch, lengths in dataloader:\n",
    "            text_batch = text_batch.to(device)\n",
    "            label_batch = label_batch.to(device).float()  # 确保标签是浮点类型\n",
    "            lengths = lengths.to(device)\n",
    "            pred = model(text_batch, lengths)[:, 0]\n",
    "            loss = loss_fn(pred, label_batch)\n",
    "            total_acc += ((pred>=0.5).float() == label_batch).float().sum().item()\n",
    "            total_loss += loss.item()*label_batch.size(0)\n",
    "            # 收集所有预测分数和真实标签\n",
    "            all_labels.extend(label_batch.cpu().numpy())\n",
    "            all_preds.extend(pred.cpu().numpy())\n",
    "    return total_acc/len(dataloader.dataset), total_loss/len(dataloader.dataset), all_labels, all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 accuracy: 0.6117 val_accuracy: 0.6420\n",
      "Epoch 1 accuracy: 0.6414 val_accuracy: 0.7438\n",
      "Epoch 2 accuracy: 0.8025 val_accuracy: 0.8212\n",
      "Epoch 3 accuracy: 0.8752 val_accuracy: 0.8482\n",
      "Epoch 4 accuracy: 0.9109 val_accuracy: 0.8534\n",
      "Epoch 5 accuracy: 0.9301 val_accuracy: 0.8362\n",
      "Epoch 6 accuracy: 0.9507 val_accuracy: 0.8698\n",
      "Epoch 7 accuracy: 0.9637 val_accuracy: 0.8652\n",
      "Epoch 8 accuracy: 0.9772 val_accuracy: 0.8670\n",
      "Epoch 9 accuracy: 0.9850 val_accuracy: 0.8718\n"
     ]
    }
   ],
   "source": [
    "#编写损失函数\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10 \n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "#开始显卡炼丹 \n",
    "for epoch in range(num_epochs):\n",
    "    acc_train, loss_train = train(train_dl)\n",
    "    acc_valid, loss_valid,_,_ = evaluate(valid_dl)\n",
    "    print(f'Epoch {epoch} accuracy: {acc_train:.4f} val_accuracy: {acc_valid:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.891\n",
      "Recall: 0.805\n",
      "F1 Score: 0.846\n",
      "AUC Score: 0.932\n"
     ]
    }
   ],
   "source": [
    "#用sklearn评估测试集效果\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "acc_test, loss_test, true_labels, pred_scores= evaluate(test_dl)\n",
    "\n",
    "# 二分类的阈值设置为0.5\n",
    "pred_labels = [1 if score >= 0.5 else 0 for score in pred_scores]\n",
    "precision = precision_score(true_labels, pred_labels)\n",
    "recall = recall_score(true_labels, pred_labels)\n",
    "f1 = f1_score(true_labels, pred_labels)\n",
    "auc = roc_auc_score(true_labels, pred_scores)\n",
    "print(f'Precision: {precision:.3f}')\n",
    "print(f'Recall: {recall:.3f}')\n",
    "print(f'F1 Score: {f1:.3f}')\n",
    "print(f'AUC Score: {auc:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
