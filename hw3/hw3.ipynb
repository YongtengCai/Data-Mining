{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 微博评论情感分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 1. 中文文本预处理——结巴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\Barry\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.390 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "# 利用中文分词工具jieba对文本进行分词，移除标点符号，最后将分词结果写入文件\n",
    "import jieba\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 读取CSV文件\n",
    "df = pd.read_csv('./weibo_senti_6k.csv', encoding='utf-8')\n",
    "\n",
    "# 定义移除标点符号的函数\n",
    "def remove_punctuation(text):\n",
    "    return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "# 对文本进行分词，并移除标点符号\n",
    "def preprocess_text(text):\n",
    "    text = remove_punctuation(text)\n",
    "    words = jieba.cut(text)\n",
    "    return ' '.join(words)\n",
    "\n",
    "# 对数据集中的每一行进行处理\n",
    "df['processed_text'] = df['review'].apply(preprocess_text)\n",
    "\n",
    "# 移除原来的review列\n",
    "df = df.drop(columns=['review'])\n",
    "\n",
    "# 将分词结果写入文件\n",
    "df.to_csv('processed_weibo_senti_6k.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4800, 27066) (1200, 27066)\n"
     ]
    }
   ],
   "source": [
    "#按4:1 划分训练集和测试集，利用向量空间模型，采用TF-IDF 权重，对预处理后的微博内容进行向量化表示\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 读取预处理后的数据集\n",
    "df = pd.read_csv('processed_weibo_senti_6k.csv', encoding='utf-8')\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['processed_text'], df['label'], test_size=0.2, random_state=1)\n",
    "\n",
    "# 初始化TF-IDF模型\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "\n",
    "# 对训练集和测试集分别进行向量化表示\n",
    "X_train_tfidf = tfidf_vec.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vec.transform(X_test)\n",
    "print(X_train_tfidf.shape, X_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 逻辑回归 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.903 Recall: 0.891\n",
      "F1 score: 0.897\n",
      "AUC score: 0.946\n"
     ]
    }
   ],
   "source": [
    "#运用逻辑回归模型开展情感分析并进行效果评价\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "#初始化逻辑回归模型\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "#训练模型\n",
    "logreg.fit(X_train_tfidf, y_train)\n",
    "\n",
    "#预测\n",
    "y_hat_test = logreg.predict(X_test_tfidf)\n",
    "print(\"Precision: {:.3f}\".format(precision_score(y_test, y_hat_test)), \"Recall: {:.3f}\".format(recall_score(y_test, y_hat_test)))\n",
    "print(\"F1 score: {:.3f}\".format(f1_score(y_test, y_hat_test)))\n",
    "y_hat_test = logreg.predict_proba(X_test_tfidf)\n",
    "print(\"AUC score: {:.3f}\".format(roc_auc_score(y_test, y_hat_test[:,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. SnowNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.554 Recall: 0.854\n",
      "F1 score: 0.672\n",
      "AUC score: 0.601\n"
     ]
    }
   ],
   "source": [
    "#使用SnowNLP自带的sentiment 函数直接开展情感分析\n",
    "\n",
    "from snownlp import SnowNLP\n",
    "\n",
    "# 读取数据集\n",
    "df = pd.read_csv('./weibo_senti_6k.csv', encoding='utf-8')\n",
    "\n",
    "# 定义情感分析函数\n",
    "def sentiment_transfer(text):\n",
    "    s = SnowNLP(text)\n",
    "    return 1 if s.sentiments > 0.5 else 0\n",
    "\n",
    "# 使用SnowNLP进行情感分析\n",
    "df['sentiment'] = df['review'].apply(sentiment_transfer)\n",
    "df['sentiment_prob']  = df['review'].apply(lambda x: SnowNLP(x).sentiments)\n",
    "\n",
    "#评估情感分析效果,计算Precision, Recall, F1 score, AUC score\n",
    "print(\"Precision: {:.3f}\".format(precision_score(df['label'], df['sentiment'])), \"Recall: {:.3f}\".format(recall_score(df['label'], df['sentiment'])))\n",
    "print(\"F1 score: {:.3f}\".format(f1_score(df['label'], df['sentiment'])))\n",
    "print(\"AUC score: {:.3f}\".format(roc_auc_score(df['label'], df['sentiment_prob'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. SBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "254a67099c24474c92fac1898e09f32b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Barry\\.cache\\huggingface\\hub\\models--sentence-transformers--paraphrase-mpnet-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40c5bee772564b13983fab6abf717243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80dbd77f218d4b17b65a321d3aebdcd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/3.73k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f410153c620d4dd99c5a124483b39045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53737b0511424f359d08c80c8f5ce405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/594 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8972c51931ba4159a373dc5d4b89d44e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8223f28948a4e42ab00b03cecd7a1e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a86c273f2b6b400f865d8b730f33acf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a66e8a29ecb24873905fb4de8c05e1f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50f3b3e70d8744aeafb4acdf8bc82cfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e41d81e01f2444f8be72a2532f70f40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4800, 768) (1200, 768)\n",
      "Precision: 0.659 Recall: 0.684\n",
      "F1 score: 0.672\n",
      "AUC score: 0.727\n"
     ]
    }
   ],
   "source": [
    "#使用SBERT对微博内容进行向量化表示\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 读取数据集\n",
    "df = pd.read_csv('./weibo_senti_6k.csv', encoding='utf-8')\n",
    "X = []\n",
    "for index, row in df.iterrows():\n",
    "    review = row['review']\n",
    "    X.append(review)\n",
    "y = df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# 加载SBERT模型\n",
    "model = SentenceTransformer('paraphrase-mpnet-base-v2')\n",
    "\n",
    "X_train_sbert = model.encode(X_train)\n",
    "X_test_sbert = model.encode(X_test)\n",
    "print(X_train_sbert.shape, X_test_sbert.shape)\n",
    "\n",
    "#初始化逻辑回归模型\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "#训练模型\n",
    "logreg.fit(X_train_sbert, y_train)\n",
    "\n",
    "#预测\n",
    "y_hat_test = logreg.predict(X_test_sbert)\n",
    "\n",
    "print(\"Precision: {:.3f}\".format(precision_score(y_test, y_hat_test)), \"Recall: {:.3f}\".format(recall_score(y_test, y_hat_test)))\n",
    "print(\"F1 score: {:.3f}\".format(f1_score(y_test, y_hat_test)))\n",
    "\n",
    "y_hat_test = logreg.predict_proba(X_test_sbert)\n",
    "print(\"AUC score: {:.3f}\".format(roc_auc_score(y_test, y_hat_test[:,1])))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
